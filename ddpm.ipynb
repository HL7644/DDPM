{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyungmoonko/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#library for data processing\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "import itertools\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import inspect\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import PIL\n",
    "import scipy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import einops\n",
    "import transformers\n",
    "import diffusers\n",
    "import accelerate\n",
    "#import clip\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "accelerator=accelerate.Accelerator()\n",
    "device=accelerator.device\n",
    "\n",
    "base_dir=\"<PATH TO SAVE>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FID module to compute FID score for generated images during training\n",
    "class FID_Module(nn.Module):\n",
    "  def __init__(self, train_data):\n",
    "    super(FID_Module, self).__init__()\n",
    "    self.train_data=train_data\n",
    "    self.inception =torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True).to(device)\n",
    "    #register hook before the final classifier\n",
    "    self.inception.Mixed_7c.register_forward_hook(self.hook_wo_fc)\n",
    "    self.avg_pool=nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "\n",
    "    self.true_mean, self.true_cov=self.get_true_images_mean_cov(train_data) #2048-d vectors\n",
    "  \n",
    "  def get_frechet_distance(self, mean, cov):\n",
    "    #mean vectors, cov matrices: batch size should be equal.\n",
    "    #test set is for generalization => we want to know how well the distribution is estimated\n",
    "    #take FID score for whle 50k traindata\n",
    "\n",
    "    diff = (mean - self.true_mean).cpu().numpy()\n",
    "    cov=cov.cpu().numpy()\n",
    "    true_cov=self.true_cov.cpu().numpy()\n",
    "    # Product might be almost singular\n",
    "    #be aware of negative elemnts in cov products.\n",
    "    covmean, _ = scipy.linalg.sqrtm(cov.dot(true_cov), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(cov.shape[0]) * eps\n",
    "        covmean = scipy.linalg.sqrtm((cov + offset).dot(true_cov + offset))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(cov)\n",
    "            + np.trace(true_cov) - 2 * tr_covmean)\n",
    "  \n",
    "  def get_mean_cov(self, inception_output):\n",
    "    #inception output: (B,2048)\n",
    "    #mean vector: (2048,)\n",
    "    #cov matrix: (2048,2048)\n",
    "    mean=torch.mean(inception_output, dim=0)\n",
    "    cov=torch.cov(inception_output.permute(1,0))\n",
    "    return mean, cov\n",
    "\n",
    "  def hook_wo_fc(self, module, input, output):\n",
    "    self.wo_fc_output=output\n",
    "    return\n",
    "  \n",
    "  def get_true_images_mean_cov(self, train_data):\n",
    "    batch_size=256\n",
    "    output_list=[]\n",
    "    #preprocess train data into (299,299)\n",
    "    fid_preprocess = torchvision.transforms.Compose([\n",
    "      torchvision.transforms.Resize(299),\n",
    "      torchvision.transforms.CenterCrop(299),\n",
    "    ])\n",
    "    train_loader=torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    pbar=tqdm(desc=\"Mean, Cov for Train Images\", total=len(train_data))\n",
    "    #no need to normalize, [0,1] for FID score computation\n",
    "    for batch_data in train_loader:\n",
    "      x, labels=batch_data\n",
    "      x=fid_preprocess(x)\n",
    "      x=x.to(device)\n",
    "      _, output=self.get_output(x)\n",
    "      output_list.append(output)\n",
    "      pbar.update(batch_size)\n",
    "    pbar.close()\n",
    "    whole_output=torch.cat(output_list, dim=0)\n",
    "    true_mean, true_cov=self.get_mean_cov(whole_output)\n",
    "    return true_mean, true_cov\n",
    "\n",
    "  def get_output(self, x):\n",
    "    with torch.no_grad():\n",
    "      incep_output=self.inception(x).logits\n",
    "      wo_fc_output=self.avg_pool(self.wo_fc_output)\n",
    "      wo_fc_output=wo_fc_output.reshape(x.size(0), 2048)\n",
    "    return incep_output, wo_fc_output\n",
    "  \n",
    "  def get_inception_score(self, incep_output):\n",
    "    #incep output: (B,1000)\n",
    "    batch_incep_probs=incep_output.softmax(dim=-1)\n",
    "    incep_marginal=torch.mean(batch_incep_probs, dim=0)\n",
    "    avg_kl_div=0\n",
    "    for incep_probs in batch_incep_probs:\n",
    "      #has numerical errors at this moment.\n",
    "      incep_categorical=torch.distributions.categorical.Categorical(probs=incep_probs)\n",
    "      marginal_categorical=torch.distributions.categorical.Categorical(probs=incep_marginal)\n",
    "      kl_div=torch.distributions.kl.kl_divergence(incep_categorical, marginal_categorical)\n",
    "      avg_kl_div=avg_kl_div+(kl_div.item()-avg_kl_div)\n",
    "    incep_score=np.exp(avg_kl_div)\n",
    "    return incep_score\n",
    "  \n",
    "  def forward(self, gen_images):\n",
    "    #get FID score\n",
    "    batch_size=256\n",
    "    #preprocess\n",
    "    fid_preprocess = torchvision.transforms.Compose([\n",
    "      torchvision.transforms.Resize(299),\n",
    "      torchvision.transforms.CenterCrop(299),\n",
    "    ])\n",
    "    gen_images=fid_preprocess(gen_images)\n",
    "\n",
    "    gen_loader=torch.utils.data.DataLoader(gen_images, batch_size=batch_size, shuffle=True)\n",
    "    incep_output_list=[]\n",
    "    wo_fc_output_list=[]\n",
    "    pbar=tqdm(desc=\"\", total=gen_images.size(0))\n",
    "    #getting UNet output of generated images\n",
    "    for gen_batch in gen_loader:\n",
    "      incep_output, wo_fc_output=self.get_output(gen_images)\n",
    "      incep_output_list.append(incep_output)\n",
    "      wo_fc_output_list.append(wo_fc_output)\n",
    "      pbar.update(batch_size)\n",
    "    whole_incep_output=torch.cat(incep_output_list, dim=0)\n",
    "    whole_wo_fc_output=torch.cat(wo_fc_output_list, dim=0)\n",
    "    #computing FID score\n",
    "    gen_mean, gen_cov=self.get_mean_cov(whole_wo_fc_output)\n",
    "    fid=self.get_frechet_distance(gen_mean, gen_cov)\n",
    "    incep_score=self.get_inception_score(whole_incep_output)\n",
    "    return incep_score, fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDPM Model\n",
    "class DDPM(nn.Module):\n",
    "  def __init__(self, betas, image_size, train_time_steps=1000, ddim_steps=20):\n",
    "    super(DDPM, self).__init__()\n",
    "    self.img_h=image_size[0]\n",
    "    self.img_w=image_size[1]\n",
    "    #train time steps\n",
    "    self.train_time_steps=train_time_steps\n",
    "    self.ddim_steps=ddim_steps\n",
    "    self.ddpm_timesteps=torch.arange(1, train_time_steps+1, 1).to(device) #should be in range of [1,T]\n",
    "    #CIFAR10 uses quadratic trajectoryes\n",
    "    c_value=(train_time_steps-1)/(ddim_steps-1)**2\n",
    "    dti=[int(c_value*(i**2)) for i in range(0, ddim_steps)]\n",
    "    self.ddim_timestep_indices=torch.LongTensor(dti).to(device)\n",
    "\n",
    "    #unet for predicting epsilon vector => the only learnable paramters\n",
    "    #model_id = \"google/ddpm-cifar10-32\"\n",
    "    #pretrained = diffusers.DDPMPipeline.from_pretrained(model_id)\n",
    "    #self.unet=pretrained.unet.to(device) #using pretrained UNet\n",
    "    self.unet=diffusers.UNet2DModel(sample_size=image_size, ).to(device)\n",
    "    \n",
    "    #forward process variance schedule\n",
    "    self.betas=betas\n",
    "    self.alphas=1-betas\n",
    "    self.sqrt_alphas=torch.sqrt(self.alphas)\n",
    "    self.inv_sqrt_alphas=1/self.sqrt_alphas\n",
    "    #alpha_bar_t\n",
    "    self.alpha_bars=torch.cumprod(self.alphas, dim=0)\n",
    "    self.sqrt_alpha_bars=torch.sqrt(self.alpha_bars)\n",
    "    self.sqrt_one_minus_alpha_bars=torch.sqrt(1-self.alpha_bars)\n",
    "    #alpha_bar_(t-1) => used for computing beta_tilde => to alpha_bars: add 1 to first index and push 1 index right\n",
    "    self.alpha_bars_prev=torch.zeros_like(self.alpha_bars).to(device)\n",
    "    self.alpha_bars_prev[0]=1\n",
    "    self.alpha_bars_prev[1:]=self.alpha_bars[:-1]\n",
    "    self.sqrt_alpha_bars_prev=torch.sqrt(self.alpha_bars_prev)\n",
    "    self.beta_tildes=((1-self.alpha_bars_prev)/(1-self.alpha_bars))*self.betas\n",
    "\n",
    "    #set variance for backward process as beta_tilde or betas'\n",
    "    self.backward_std_hat=torch.sqrt(self.betas)\n",
    "    self.backward_std=torch.sqrt(self.beta_tildes)\n",
    "  \n",
    "  def normalize(self, tensor):\n",
    "    #[0,1] => [-1,1]\n",
    "    #mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    return tensor*2-1\n",
    "  \n",
    "  def unnormalize(self, tensor):\n",
    "    #[-1,1] => [0,1]\n",
    "    #mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    return (tensor+1)*0.5\n",
    "    \n",
    "  #used for training => same as q_sampling but with estimated noise vector\n",
    "  def get_x_t_vector(self, x_0, timesteps, epsilon):\n",
    "    #modeling the forward process from x_0 -> x_t =>\n",
    "    x_0_coeff=torch.gather(self.sqrt_alpha_bars, 0, timesteps).reshape(timesteps.size(0),1,1,1)\n",
    "    eps_coeff=torch.gather(self.sqrt_one_minus_alpha_bars, 0, timesteps).reshape(timesteps.size(0),1,1,1)\n",
    "    x_t=x_0_coeff*x_0+eps_coeff*epsilon\n",
    "    return x_t\n",
    "  \n",
    "  #rest used for sequential sampling\n",
    "  def q_posterior_sample(self, x_t, x_0, timestep_idx):\n",
    "    batch_size=x_0.size(0)\n",
    "    normal_vector=torch.randn_like(x_0).to(device)\n",
    "    posterior_mean=self.get_posterior_mean(x_t, x_0, timestep_idx)\n",
    "    x_prev=posterior_mean+self.beta_tildes[timestep_idx]*normal_vector\n",
    "    return x_prev\n",
    "  \n",
    "  def get_x_0_vector(self, x_t, timestep_idx, epsilon):\n",
    "    x_0_coeff=self.sqrt_alpha_bars[timestep_idx]\n",
    "    eps_coeff=self.sqrt_one_minus_alpha_bars[timestep_idx]\n",
    "    x_0=(x_t-eps_coeff*epsilon)/x_0_coeff\n",
    "    return x_0\n",
    "  \n",
    "  #used to demonstrate forward process\n",
    "  def q_sample(self, x_0, timestep_idx):\n",
    "    #estimating x_t from x_0 using q(x_t|x_t-1)\n",
    "    batch_size=x_0.size(0)\n",
    "    normal_vector=torch.randn_like(x_0).to(device)\n",
    "    x_t=self.sqrt_alpha_bars[timestep_idx]*x_0+self.sqrt_one_minus_alpha_bars[timestep_idx]*normal_vector\n",
    "    return x_t\n",
    "\n",
    "  def get_backward_mean(self, x_t, timestep_idx, epsilon):\n",
    "    mu_vector=self.inv_sqrt_alphas[timestep_idx]*(x_t-(self.betas[timestep_idx]/self.sqrt_one_minus_alpha_bars[timestep_idx])*epsilon)\n",
    "    return mu_vector\n",
    "\n",
    "  def p_sample(self, x_t, timestep_idx):\n",
    "    #x_(t-1)=N(x_t; mu(x_t, t), sigma_t*I)\n",
    "    timestep=self.ddpm_timesteps[timestep_idx]\n",
    "    timestep_tensor=torch.full([x_t.size(0)], timestep).to(device)\n",
    "    eps_pred=self.forward(x_t, timestep_tensor) #pretrained UNet gives almost gaussian tensor.\n",
    "    mu_vector=self.get_backward_mean(x_t, timestep_idx, eps_pred)\n",
    "    if timestep==0:\n",
    "      normal_vector=torch.zeros_like(x_t).to(device)\n",
    "    else:\n",
    "      normal_vector=torch.randn_like(x_t).to(device)\n",
    "    x_prev=mu_vector+self.backward_std_hat[timestep_idx]*normal_vector\n",
    "    return x_prev\n",
    "  \n",
    "  def ddim_sample(self, x_t, idx, next_idx):\n",
    "    timestep_tensor=torch.full([x_t.size(0)], self.ddpm_timesteps[idx]).to(device)\n",
    "    #timestep idx in DDPM lists => ddim timstep -1\n",
    "    eps_pred=self.forward(x_t, timestep_tensor)\n",
    "    normal_vector=torch.randn_like(x_t).to(device)\n",
    "    x_0=self.get_x_0_vector(x_t, idx, eps_pred)\n",
    "    if idx==0:\n",
    "      #directly sample x_0 from x_t\n",
    "      return x_0\n",
    "    else:\n",
    "      #alpha_bars_prev=alpha_bars[next_idx] => b.c. ddim steps skipping steps => alpha_bars_prev's index !=idx-1\n",
    "      eps_coeff=torch.sqrt(1-self.alpha_bars[next_idx]-self.ddim_std[idx]**2)\n",
    "      x_prev=self.sqrt_alpha_bars_prev[next_idx]*x_0+eps_coeff*eps_pred+self.ddim_std[idx]*normal_vector\n",
    "      return x_prev\n",
    "  \n",
    "  def generate(self, batch_size, mode=\"ddpm\", ddim_eta=None, verbose=True):\n",
    "    assert mode in ['ddpm', 'ddim'], \"Invalid Mode, Valid Modes: ddpm, ddim\"\n",
    "    assert (mode==\"ddim\" and ddim_eta!=None) or (mode!='ddim' and ddim_eta==None), \"DDIM eta should be provided\"\n",
    "    #ddim with eta=1 => DDPM, eta=0 => DDIM (deterministic)\n",
    "    x=torch.randn(batch_size, 3, self.img_w, self.img_w).to(device)\n",
    "    #iterate through T steps to generate unconditionally\n",
    "    inference_steps=self.train_time_steps if mode==\"ddpm\" else self.ddim_steps\n",
    "    if mode=='ddim':\n",
    "      self.ddim_std=ddim_eta*self.backward_std\n",
    "    else:\n",
    "      self.ddim_std=self.backward_std\n",
    "    if verbose:\n",
    "      pbar=tqdm(desc=\"Unconditional Generation\", total=inference_steps)\n",
    "    for timestep_idx in np.arange(inference_steps-1, -1, -1):\n",
    "      #predicting x_(t-1)\n",
    "      with torch.no_grad():\n",
    "        if mode==\"ddpm\":\n",
    "          x=self.p_sample(x, timestep_idx)\n",
    "          #x=self.ddim_sample(x, timestep_idx)\n",
    "        else:\n",
    "          idx=self.ddim_timestep_indices[timestep_idx] #steps in DDPM corresponding to DDIM\n",
    "          if timestep_idx==0:\n",
    "            next_idx=0\n",
    "          else:            \n",
    "            next_idx=self.ddim_timestep_indices[timestep_idx-1]\n",
    "          x=self.ddim_sample(x, idx, next_idx)\n",
    "      if verbose:\n",
    "        pbar.update(1)\n",
    "    x=self.unnormalize(x)\n",
    "    x=torch.clamp(x, min=0, max=1)\n",
    "    if verbose:\n",
    "      pbar.close()\n",
    "    return x\n",
    "\n",
    "  def forward(self, x_t, timesteps):\n",
    "    #can be processed in batch x_t : b,c,h,w & timesteps: b,\n",
    "    #predicting eps(x_t, t), timesteps ranging [0, T-1]\n",
    "    eps_pred=self.unet(x_t, timesteps).sample\n",
    "    #eps_pred: same shape as x_t\n",
    "    return eps_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM_Trainer():\n",
    "  def __init__(self, data, image_size, num_train_steps, ddim_steps, beta_low, beta_high):\n",
    "    self.image_size=image_size\n",
    "    #pipeline = diffusers.DiffusionPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\n",
    "    #self.unet=pipeline.unet.to(device)\n",
    "    #ddpm schedule parameters\n",
    "    self.num_train_steps=num_train_steps\n",
    "    self.ddim_steps=ddim_steps\n",
    "    self.betas=betas=torch.linspace(beta_low, beta_high, num_train_steps).to(device)\n",
    "\n",
    "    self.ddpm=DDPM(self.betas, image_size, num_train_steps, ddim_steps)\n",
    "    self.fid_module=FID_Module(data['train'])\n",
    "\n",
    "    #testing with CIFAR 10\n",
    "    self.train_data=data['train']\n",
    "    self.test_data=data['test']\n",
    "\n",
    "    self.train_logs={\n",
    "        'loss_history': [],\n",
    "        'val_fid_history': [],\n",
    "        'is_history': [],\n",
    "        'validation_images': [],\n",
    "    }\n",
    "  \n",
    "  def show_noising_image(self):\n",
    "    train_loader=torch.utils.data.DataLoader(self.train_data, batch_size=4, shuffle=True)\n",
    "    milestones=[int(a*self.num_train_steps) for a in [0.25, 0.5, 0.75, 1]]\n",
    "    images=[[] for _ in milestones]\n",
    "    for batch_data in train_loader:\n",
    "      x, _ =batch_data\n",
    "      x=self.ddpm.normalize(x)\n",
    "      x=x.to(device)\n",
    "      #run forward process\n",
    "      for timestep in range(self.num_train_steps):\n",
    "        x=self.ddpm.q_sample(x, timestep)\n",
    "      break\n",
    "    #display images\n",
    "    plt.figure()\n",
    "  \n",
    "  def plot_train_logs(self):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    #plot loss\n",
    "    plt.subplot(1,3,1)\n",
    "    lh=self.train_logs['loss_history']\n",
    "    plt.plot(np.arange(1, len(lh+1), 1), lh)\n",
    "    plt.xlabel(\"Update steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    #plot FID scores\n",
    "    plt.subplot(1,3,2)\n",
    "    fidh=self.train_logs['val_fid_history']\n",
    "    plt.plot(np.arange(1, len(fidh)+1, 1), fidh)\n",
    "    plt.xlabel(\"Validation steps\")\n",
    "    plt.ylabel(\"FID scores\")\n",
    "\n",
    "    #plot IS scores\n",
    "    plt.subplot(1,3,3)\n",
    "    ish=self.train_logs['is_history']\n",
    "    plt.plot(np.arange(1, len(ish)+1, 1), ish)\n",
    "    plt.xlabel(\"Validation steps\")\n",
    "    plt.ylabel(\"FID scores\")\n",
    "\n",
    "    plt.show()\n",
    "    return\n",
    "  \n",
    "  def save_train_logs(self, name):\n",
    "    with open(os.path.join(base_dir, \"train_logs/{:s}.pkl\".format(name)), 'wb') as file:\n",
    "      pickle.dump(self.train_logs, file)\n",
    "    #also save model\n",
    "    torch.save(self.ddpm.state_dict(), os.path.join(base_dir, \"models/{:s}.pkl\".format(name)))\n",
    "    return\n",
    "\n",
    "  def get_val_sizes(self, val_batch_size, len_validation):\n",
    "    if len_validation%val_batch_size==0:\n",
    "      length=int(len_validation/val_batch_size)\n",
    "      val_sizes=[val_batch_size for _ in range(length)]\n",
    "    else:\n",
    "      length=len_validation//val_batch_size\n",
    "      val_sizes=[val_batch_size for _ in range(length)]\n",
    "      val_sizes.append(int(len_validation-length*val_batch_size))\n",
    "    return val_sizes\n",
    "  \n",
    "  def validate(self, epoch, val_batch_size, len_validation):\n",
    "    #generate images and observe quality of images qualitatively.\n",
    "    gen_images_list=[]\n",
    "    #generate 50k images\n",
    "    pbar=tqdm(desc=\"Validating\", total=len_validation)\n",
    "    val_sizes=self.get_val_sizes(val_batch_size, len_validation)\n",
    "    for val_size in val_sizes:\n",
    "      #apply DDIM sampling for efficiency.\n",
    "      gen_images=self.ddpm.generate(val_size, mode=\"ddim\", ddim_eta=0, verbose=False)\n",
    "      gen_images_list.append(gen_images)\n",
    "      pbar.update(val_size)\n",
    "    pbar.close()\n",
    "    whole_gen_images=torch.cat(gen_images_list, dim=0)\n",
    "    #get fid score\n",
    "    incep_score, fid_score=self.fid_module(whole_gen_images)\n",
    "    #show some images\n",
    "    print(\"Validation for Epoch: {:d}, IS: {:.3f}, FID score: {:.3f}\".format(epoch, incep_score, fid_score.item()))\n",
    "    plt.figure(figsize=(20,5))\n",
    "    for idx in range(16):\n",
    "      plt.subplot(2,8,idx+1)\n",
    "      plt.imshow(whole_gen_images[idx].permute(1,2,0).cpu().numpy())\n",
    "    plt.show()\n",
    "    self.train_logs['validation_images'].append(whole_gen_images[:100]) #(1000,3,32,32) => large size => store only 100 images.\n",
    "    self.train_logs['is_history'].append(incep_score)\n",
    "    self.train_logs['val_fid_history'].append(fid_score.item())\n",
    "    return incep_score, fid_score\n",
    "  \n",
    "  def show_generation(self, epoch):\n",
    "    print(\"Validation for Epoch: {:d}\".format(epoch))\n",
    "    gen_images=self.ddpm.generate(16, verbose=False)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    for idx in range(16):\n",
    "      plt.subplot(2, 8, idx+1)\n",
    "      plt.imshow(gen_images[idx].permute(1,2,0).cpu().numpy())\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "  def test(self):\n",
    "    return\n",
    "  \n",
    "  def get_loss(self, x_0):\n",
    "    batch_size, ch, img_h, img_w=x_0.size()\n",
    "    #loss ftn: MSE Loss\n",
    "    mse_loss=nn.MSELoss(reduction=\"mean\")\n",
    "    normal_vector=torch.randn_like(x_0).to(device)\n",
    "    timestep_indices=torch.randint(low=0, high=self.num_train_steps, size=[batch_size]).to(device)\n",
    "    timesteps=torch.gather(self.ddpm.ddpm_timesteps, 0, timestep_indices)\n",
    "    #x_t: corrupted image from x_0\n",
    "    x_t=self.ddpm.get_x_t_vector(x_0, timestep_indices, normal_vector)\n",
    "    #estimated noise conditioned on timestep from x_t\n",
    "    pred_eps=self.ddpm(x_t, timesteps)\n",
    "    loss=mse_loss(pred_eps, normal_vector)\n",
    "    return loss\n",
    "  \n",
    "  def train(self, num_epochs, batch_size, lr, reg, validate_every, val_batch_size, len_validation):\n",
    "    #training the UNet network of the DDPM\n",
    "    optimizer=optim.AdamW(self.ddpm.parameters(), lr=lr, weight_decay=reg)\n",
    "    train_loader=torch.utils.data.DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "    update_steps=(len(self.train_data)//batch_size)*num_epochs if len(self.train_data)%batch_size==0 else (len(self.train_data)//batch_size+1)*num_epochs\n",
    "    #horizontal flip operation\n",
    "    horiz_flip=torchvision.transforms.RandomHorizontalFlip(p=0.5)\n",
    "    pbar=tqdm(desc=\"DDPM Training Update Steps\", total=update_steps)\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "      for b_idx, batch_data in enumerate(train_loader):\n",
    "        x_0, _=batch_data\n",
    "        x_0=x_0.to(device)\n",
    "        x_0=horiz_flip(x_0)\n",
    "        x_0=self.ddpm.normalize(x_0) #normalize to range of [-1,1]\n",
    "        \n",
    "        loss=self.get_loss(x_0)\n",
    "        self.train_logs['loss_history'].append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.update(1)\n",
    "      if epoch%validate_every==0:\n",
    "        #incep_score, fid=self.validate(epoch, val_batch_size, len_validation)\n",
    "        #print(incep_score, fid)\n",
    "        self.show_generation(epoch)\n",
    "    #self.test()\n",
    "    pbar.close()\n",
    "    return self.train_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/train/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243fa87ffe54457da790c78451e8b8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train/cifar-10-python.tar.gz to data/train\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/test/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a1b6cf0561460e806118df262f9e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/test/cifar-10-python.tar.gz to data/test\n"
     ]
    }
   ],
   "source": [
    "#get CIFAR 10 dataset\n",
    "cifar_preprocess=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "cifar10_train=torchvision.datasets.CIFAR10(root='data/train', train=True, transform=cifar_preprocess, download=True)\n",
    "small_cifar10_train=torch.utils.data.Subset(cifar10_train, np.arange(0, 5000, 1))\n",
    "cifar10_test=torchvision.datasets.CIFAR10(root='data/test', train=False, transform=cifar_preprocess, download=True)\n",
    "#use full data for training\n",
    "small_cifar10={\n",
    "    'train': small_cifar10_train,\n",
    "    'test': cifar10_test\n",
    "}\n",
    "cifar10={\n",
    "    'train': cifar10_train,\n",
    "    'test': cifar10_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=DDPM_Trainer(data=cifar10, image_size=(32,32), num_train_steps=1000, ddim_steps=20, beta_low=1e-4, beta_high=0.2)\n",
    "#incompatible with MPS\n",
    "train_logs=trainer.train(\n",
    "    num_epochs=100,\n",
    "    batch_size=128,\n",
    "    lr=2e-4,\n",
    "    reg=0,\n",
    "    validate_every=10,\n",
    "    val_batch_size=128,\n",
    "    len_validation=10000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "641a7458bfae2bc959d7f867e9e3882167acabe29543290f7c5231fa0d54378e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
